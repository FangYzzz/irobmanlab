# -*- coding: utf-8 -*-
"""OWL_ViT_minimal_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb

# OWL-ViT minimal example

This Colab shows how to **load a pre-trained OWL-ViT checkpoint** and use it to
**get object detection predictions** for an image.

# Download and install OWL-ViT

OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it.
"""

# !rm -rf *
# !rm -rf .config
# !rm -rf .git
# !git clone https://github.com/google-research/scenic.git .
# !python -m pip install -q .
# !python -m pip install -r ./scenic/projects/owl_vit/requirements.txt

# Also install big_vision, which is needed for the mask head:
# !mkdir /big_vision
# !git clone https://github.com/google-research/big_vision.git /big_vision
# !python -m pip install -r /big_vision/big_vision/requirements.txt
import sys
import os     # 11111

sys.path.append('big_vision/')
sys.path.append('/home/yuan/Mani-GPT/Detic')
# !echo "Done."


import jax
from matplotlib import pyplot as plt
import numpy as np
from scenic.projects.owl_vit import configs
from scenic.projects.owl_vit import models
from scipy.special import expit as sigmoid
import skimage
from skimage import io as skimage_io
from skimage import transform as skimage_transform

import io
from PIL import Image

import matplotlib.pyplot as plt

# import camera_capture
# import Detic.demo
# import GPT
import point_cloud_grasp_sample

import tensorflow as tf


def prediction(module, variables, input_image, tokenized_queries, text_queries, crop, cx_=0, cy_=0, original_padding_image=None, width_ =0, height_=0):
    
    jitted = jax.jit(module.apply, static_argnames=('train',)) 
    
    # Note: The model expects a batch dimension.
    predictions = jitted(
        variables,
        input_image[None, ...],
        tokenized_queries[None, ...],
        train=False)

    # Remove batch dimension and convert to numpy:
    predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions)


    """# Plot predictions"""

    score_threshold = 0.2   # initial 0.2

    logits = predictions['pred_logits'][..., :len(text_queries)]       # Remove padding
    scores = sigmoid(np.max(logits, axis=-1))
    labels = np.argmax(predictions['pred_logits'], axis=-1)
    boxes = predictions['pred_boxes']


    if(crop==False):
        fig, ax = plt.subplots(1, 1, figsize=(8, 8))
        ax.imshow(original_padding_image, extent=(0, 1, 1, 0))   
        ax.set_axis_off()

    for score, box, label in zip(scores, boxes, labels):
        if score < score_threshold:
            continue
        
        cx, cy, w, h = box
        if crop == True:                       
            img_height, img_width, _ = input_image.shape
            print("original padding image:", img_height, img_width)
            print("crop = True: cx, cy, w, h: ", cx, cy, w, h)
            
            # 计算裁剪区域的左上和右下坐标
            left = int((cx - w / 2) * img_width)
            upper = int((cy - h / 2) * img_height)
            right = int((cx + w / 2) * img_width)
            lower = int((cy + h / 2) * img_height)

            # Crop the image
            cropped_image_array = input_image[upper:lower, left:right]

            # Convert the cropped NumPy array to a Pillow image object
            cropped_image = Image.fromarray((cropped_image_array * 255).astype(np.uint8))

            # cropped_image.save(os.path.join('/home/yuan/Mani-GPT/camera_capture/cropped_image_6.jpg'))
            # cropped_image.save(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_mug.png'))        #####
            cropped_image.save(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_5_pepper.png'))        #####
            # cropped_image.show()
            
            height_c, width_c,_= cropped_image_array.shape
            print("croped image:", height_c, width_c)
        
            return input_image, cx, cy, w, h, height_c, width_c
        
        else:
            cx, cy, w, h = box
            img_height, img_width, _ = input_image.shape
            img_height_, img_width_, _ = original_padding_image.shape
            # print(img_height, img_width, img_height_, img_width_)

            size = max(height_, width_)
            cx = cx*size/width_

            cx = cx_ - 0.5 * width_ / img_width_ + cx * width_ / img_width_
            cy = cy * size / height_
            cy = cy_ - 0.5 * height_ / img_height_ + cy * height_ / img_height_

            w = w * size / img_width_
            h = h * size / img_height_
            
            print("crop = False: cx, cy, w, h: ", cx, cy, w, h)
            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],
                [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')
            ax.text(
                cx - w / 2,
                cy + h / 2 + 0.015,
                f'{text_queries[label]}: {score:1.2f}',
                ha='left',
                va='top',
                color='red',
                bbox={
                    'facecolor': 'white',
                    'edgecolor': 'red',
                    'boxstyle': 'square,pad=.3'
                })
            # plt.savefig(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_mug_1.png'))       #####
            # plt.savefig(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_mug-handle_1.png'))       #####
            plt.savefig(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_5_pepper_1.png'))            #####
            # plt.savefig(os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_4_fryingpan-handle_1.png'))       #####
            plt.show()  
            return cx, cy, w, h
            
        


def detect_grasp_name(grasp_name, crop, cx_=0, cy_=0, original_padding_image=None, height=0, width=0):

    """# Choose config"""

    config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')


    """# Load the model and variables"""

    module = models.TextZeroShotDetectionModule(
        body_configs=config.model.body,
        objectness_head_configs=config.model.objectness_head,
        normalize=config.model.normalize,
        box_bias=config.model.box_bias)
    variables = module.load_variables(config.init_from.checkpoint_path)


    """# Prepare image"""

    # filepath =camera_capture.zed()                                                                         ###

    # Load example image:
    if crop == True:
        # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/6.jpeg')
        # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/ZED_mug.png')                #####
        filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/ZED_image5.png')               #####
    else:
        # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/cropped_image_6.jpg')
        # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_mug.png')            #####
        filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/lab/cropped_5_pepper.png')            #####



    image_uint8 = skimage_io.imread(filename_input)
    image = image_uint8.astype(np.float32) / 255.0
    # print(image.shape)

    if image.shape[2] == 4:                                                                              # astronaut(512,512,3) ZED(1080,1920,4)
        image = image[:, :, :3]
    # print(image.shape)

    # Pad to square with gray pixels on bottom and right:
    h, w, _ = image.shape
    size = max(h, w)
    image_padded = np.pad(
        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)

    # Resize to model input size:
    input_image = skimage.transform.resize(
        image_padded,
        (config.dataset_configs.input_size, config.dataset_configs.input_size),
        anti_aliasing=True)



    """# Prepare text queries"""

    # grasp_name = GPT.gpt_dialogue()                                               
    text_queries = [grasp_name]                                             
    # text_queries = ['fork']


    tokenized_queries = np.array([
        module.tokenize(q, config.dataset_configs.max_query_length)
        for q in text_queries
    ])

    # Pad tokenized queries to avoid recompilation if number of queries changes:
    tokenized_queries = np.pad(
        tokenized_queries,
        pad_width=((0, 100 - len(text_queries)), (0, 0)),
        constant_values=0)

    if crop== True:
        original_padding_image, cx, cy, w, h, height, width = prediction(module, variables, input_image, tokenized_queries, text_queries, crop)
        return original_padding_image, cx, cy, w, h, height, width
    else:
        ## sub fig
        cx, cy, w, h = prediction(module, variables, input_image, tokenized_queries, text_queries, crop, cx_, cy_, original_padding_image, width_=width, height_=height)
        return cx, cy, w, h




grasp_object = 'pepper'                                                             # image5_pepper
# grasp_object = 'plate'                                                              # image5_plate
# grasp_object = 'cup'                                                                # image5_cup
# grasp_object = 'tea pot'                                                            # image5_teapot
# grasp_object = 'frying pan'                                                         # image4_frying_pan
# grasp_object = 'mug'                                                                # image4_mug

# grasp_part = 'The handle'
# grasp_part = 'carrot'
    

crop = True
original_padding_image, cx, cy, w, h, height, width = detect_grasp_name(grasp_object, crop)

# crop = False
# cx, cy, w, h = detect_grasp_name(grasp_part, crop, cx, cy, original_padding_image, height, width)


grasps_pos, grasps_rot = point_cloud_grasp_sample.get_GraspSample(cx, cy, w, h)







# cx = cx                                    # 裁剪区域中心点x坐标
# cy = cy * 1920.0 / 1080.0                  # 裁剪区域中心点y坐标
# w = w                                      # 裁剪区域宽度
# h = h * 1920.0 / 1080.0                    # 裁剪区域高度
# img_width = 1920                           # 原图宽度
# img_height = 1080                          # 原图高度

# left = int((cx - w / 2) * img_width)
# upper = int((cy - h / 2) * img_height)
# right = int((cx + w / 2) * img_width)
# lower = int((cy + h / 2) * img_height)

# # 加载原图/深度图 (假设深度图是灰度图)
# # rgb_image = Image.open("/home/yuan/Mani-GPT/camera_capture/lab/ZED_mug.png")                             #####
# rgb_image = Image.open("/home/yuan/Mani-GPT/camera_capture/lab/ZED_image5.png")
# # depth_image = Image.open("/home/yuan/Mani-GPT/camera_capture/lab/Depth_mug.png")                         #####
# depth_image = Image.open("/home/yuan/Mani-GPT/camera_capture/lab/Depth_5.png")

# # 裁剪并保存原图/深度图
# cropped_rgb_image = rgb_image.crop((left, upper, right, lower))
# cropped_depth_image = depth_image.crop((left, upper, right, lower))

# # cropped_rgb_image.save("/home/yuan/Mani-GPT/camera_capture/lab/cropped_rgb_mug.png")                     #####
# cropped_rgb_image.save("/home/yuan/Mani-GPT/camera_capture/lab/cropped_rgb_5_cup.png")                     #####    cropped_rgb_image =/= cropped_image
# print("cropped_rgb_image saved")
# # cropped_depth_image.save("/home/yuan/Mani-GPT/camera_capture/lab/cropped_depth_mug.png")                 #####
# cropped_depth_image.save("/home/yuan/Mani-GPT/camera_capture/lab/cropped_depth_5_cup.png")                 #####
# print("cropped_depth_image saved")



















# """# Get predictions
# This will take a minute on the first execution due to model compilation. Subsequent executions will be faster.
# """

# def prediction(module, variables, input_image, tokenized_queries, text_queries, crop):
#     jitted = jax.jit(module.apply, static_argnames=('train',)) 

#     # Note: The model expects a batch dimension.
#     predictions = jitted(
#         variables,
#         input_image[None, ...],
#         tokenized_queries[None, ...],
#         train=False)

#     # Remove batch dimension and convert to numpy:
#     predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions)


#     """# Plot predictions"""

#     # Commented out IPython magic to ensure Python compatibility.
#     # %matplotlib inline

#     score_threshold = 0.2

#     logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.
#     scores = sigmoid(np.max(logits, axis=-1))
#     labels = np.argmax(predictions['pred_logits'], axis=-1)
#     boxes = predictions['pred_boxes']

#     fig, ax = plt.subplots(1, 1, figsize=(8, 8))
#     ax.imshow(input_image, extent=(0, 1, 1, 0))
#     ax.set_axis_off()

#     for score, box, label in zip(scores, boxes, labels):
#         if score < score_threshold:
#             continue

#         cx, cy, w, h = box
#         if crop == True:                                       # 33333
#             img_height, img_width, _ = input_image.shape

#             # 计算裁剪区域的左上和右下坐标
#             left = int((cx - w / 2) * img_width)
#             upper = int((cy - h / 2) * img_height)
#             right = int((cx + w / 2) * img_width)
#             lower = int((cy + h / 2) * img_height)

#             # 裁剪图像
#             cropped_image_array = input_image[upper:lower, left:right]

#             # 将裁剪的 NumPy 数组转换为 Pillow 图像对象
#             cropped_image = Image.fromarray((cropped_image_array * 255).astype(np.uint8))

#             # 保存裁剪的图像
#             cropped_image.save(os.path.join('/home/yuan/Mani-GPT/camera_capture/cropped_image_6.jpg'))
#             # cropped_image.show()

#             return cropped_image
        
#         else:
#             ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],
#                 [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')
#             ax.text(
#                 cx - w / 2,
#                 cy + h / 2 + 0.015,
#                 f'{text_queries[label]}: {score:1.2f}',
#                 ha='left',
#                 va='top',
#                 color='red',
#                 bbox={
#                     'facecolor': 'white',
#                     'edgecolor': 'red',
#                     'boxstyle': 'square,pad=.3'
#                 })
        
#     plt.show()    
        
#     return fig     # 11111
        

# def plot_and_return_image(fig):

#     # 使用 BytesIO 保存图像为字节流
#     buf = io.BytesIO()
#     fig.savefig(buf, format='jpeg')
#     buf.seek(0)        

#     # print(buf.getvalue())    

#     return buf.getvalue()


# def detect_grasp_name(grasp_name, crop):

#     """# Choose config"""

#     config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')


#     """# Load the model and variables"""

#     module = models.TextZeroShotDetectionModule(
#         body_configs=config.model.body,
#         objectness_head_configs=config.model.objectness_head,
#         normalize=config.model.normalize,
#         box_bias=config.model.box_bias)
#     variables = module.load_variables(config.init_from.checkpoint_path)


#     """# Prepare image"""

#     # filepath =camera_capture.zed()         # 11111
#     # filepath = camera_capture.video_demo()   # 00000

#     # Load example image:
#     # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/1.png')   # 11111
    
#     crop = crop
    
#     if crop == True:
#         filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/6.jpeg')
#     else:
#         filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/cropped_image_6.jpg')



#     image_uint8 = skimage_io.imread(filename_input)
#     image = image_uint8.astype(np.float32) / 255.0
#     # print(image.shape)

#     # Pad to square with gray pixels on bottom and right:
#     h, w, _ = image.shape
#     size = max(h, w)
#     image_padded = np.pad(
#         image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)

#     # Resize to model input size:
#     input_image = skimage.transform.resize(
#         image_padded,
#         (config.dataset_configs.input_size, config.dataset_configs.input_size),
#         anti_aliasing=True)



#     """# Prepare text queries"""

#     # grasp_name = GPT.gpt_dialogue()                                               
#     text_queries = [grasp_name]                                                    # 11111
#     # text_queries = ['saucepan', 'knife']                                         
#     # text_queries = ['fork']


#     tokenized_queries = np.array([
#         module.tokenize(q, config.dataset_configs.max_query_length)
#         for q in text_queries
#     ])

#     # Pad tokenized queries to avoid recompilation if number of queries changes:
#     tokenized_queries = np.pad(
#         tokenized_queries,
#         pad_width=((0, 100 - len(text_queries)), (0, 0)),
#         constant_values=0)

    
#     image = prediction(module, variables, input_image, tokenized_queries, text_queries, crop)
    
    
#     # Call the function and get the byte stream
#     # image_bytes = plot_and_return_image(image)                      # 55555


#     # return image_bytes                                              # 55555




# grasp_object = 'frying pan'
# grasp_part = 'The handle'
    
# # convert a byte stream to an image object
# crop = True
# # image_bytes = detect_grasp_name(grasp_object, crop)                 # 55555
# detect_grasp_name(grasp_object, crop)
# crop = False
# # image_bytes = detect_grasp_name(grasp_part, crop)                   # 55555
# detect_grasp_name(grasp_part, crop)

# # image = Image.open(io.BytesIO(image_bytes))                         # 55555

# # # show the image
# # plt.imshow(image)
# # plt.axis('off')        # Hide axes
# # plt.show()



# # detect_grasp_name(grasp_name)
# # detect_grasp_name('the handle of Frying pan')                      # 11111



# # plt.show()
# # sys.exit()