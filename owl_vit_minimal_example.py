# -*- coding: utf-8 -*-
"""OWL_ViT_minimal_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb

# OWL-ViT minimal example

This Colab shows how to **load a pre-trained OWL-ViT checkpoint** and use it to
**get object detection predictions** for an image.

# Download and install OWL-ViT

OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it.
"""

# !rm -rf *
# !rm -rf .config
# !rm -rf .git
# !git clone https://github.com/google-research/scenic.git .
# !python -m pip install -q .
# !python -m pip install -r ./scenic/projects/owl_vit/requirements.txt

# Also install big_vision, which is needed for the mask head:
# !mkdir /big_vision
# !git clone https://github.com/google-research/big_vision.git /big_vision
# !python -m pip install -r /big_vision/big_vision/requirements.txt
import sys
import os     # 11111

sys.path.append('big_vision/')
sys.path.append('/home/yuan/Mani-GPT/Detic')       # 11111
# !echo "Done."


import jax
from matplotlib import pyplot as plt
import numpy as np
from scenic.projects.owl_vit import configs
from scenic.projects.owl_vit import models
from scipy.special import expit as sigmoid
import skimage
from skimage import io as skimage_io
from skimage import transform as skimage_transform

import io
from PIL import Image

# import camera_capture
# import GPT
# import grasp
# import Detic.demo
import tensorflow as tf


"""# Get predictions
This will take a minute on the first execution due to model compilation. Subsequent executions will be faster.
"""

def prediction(module, variables, input_image, tokenized_queries, text_queries):
    jitted = jax.jit(module.apply, static_argnames=('train',)) 

    # Note: The model expects a batch dimension.
    predictions = jitted(
        variables,
        input_image[None, ...],
        tokenized_queries[None, ...],
        train=False)

    # Remove batch dimension and convert to numpy:
    predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions)


    """# Plot predictions"""

    # Commented out IPython magic to ensure Python compatibility.
    # %matplotlib inline

    score_threshold = 0.2

    logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.
    scores = sigmoid(np.max(logits, axis=-1))
    labels = np.argmax(predictions['pred_logits'], axis=-1)
    boxes = predictions['pred_boxes']

    fig, ax = plt.subplots(1, 1, figsize=(8, 8))
    ax.imshow(input_image, extent=(0, 1, 1, 0))
    ax.set_axis_off()

    for score, box, label in zip(scores, boxes, labels):
        if score < score_threshold:
            continue

        cx, cy, w, h = box
        ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],
                [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')
        ax.text(
            cx - w / 2,
            cy + h / 2 + 0.015,
            f'{text_queries[label]}: {score:1.2f}',
            ha='left',
            va='top',
            color='red',
            bbox={
                'facecolor': 'white',
                'edgecolor': 'red',
                'boxstyle': 'square,pad=.3'
            })
        
    return fig     # 11111
        

def plot_and_return_image(fig):

    # 使用 BytesIO 保存图像为字节流
    buf = io.BytesIO()
    fig.savefig(buf, format='jpeg')
    buf.seek(0)        

    # print(buf.getvalue())    

    return buf.getvalue()


def detect_grasp_part(grasp_part):

    """# Choose config"""

    config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')


    """# Load the model and variables"""

    module = models.TextZeroShotDetectionModule(
        body_configs=config.model.body,
        objectness_head_configs=config.model.objectness_head,
        normalize=config.model.normalize,
        box_bias=config.model.box_bias)
    variables = module.load_variables(config.init_from.checkpoint_path)


    """# Prepare image"""

    # filepath =camera_capture.zed()         # 11111
    # filepath = camera_capture.video_demo()   # 00000

    # Load example image:
    # filename = os.path.join(skimage.data_dir, 'astronaut.png')
    # filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/1.png')   # 11111
    filename_input = os.path.join('/home/yuan/Mani-GPT/camera_capture/6.jpeg')

    image_uint8 = skimage_io.imread(filename_input)
    image = image_uint8.astype(np.float32) / 255.0
    # print(image.shape)

    # Pad to square with gray pixels on bottom and right:
    h, w, _ = image.shape
    size = max(h, w)
    image_padded = np.pad(
        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)

    # Resize to model input size:
    input_image = skimage.transform.resize(
        image_padded,
        (config.dataset_configs.input_size, config.dataset_configs.input_size),
        anti_aliasing=True)



    """# Prepare text queries"""

    # grasp_part = GPT.gpt_dialogue()                                               
    text_queries = [grasp_part]                                                    # 11111
    # text_queries = ['saucepan', 'knife']                                         
    # text_queries = ['fork']


    tokenized_queries = np.array([
        module.tokenize(q, config.dataset_configs.max_query_length)
        for q in text_queries
    ])

    # Pad tokenized queries to avoid recompilation if number of queries changes:
    tokenized_queries = np.pad(
        tokenized_queries,
        pad_width=((0, 100 - len(text_queries)), (0, 0)),
        constant_values=0)


    fig = prediction(module, variables, input_image, tokenized_queries, text_queries)

    
    # Call the function and get the byte stream
    image_bytes = plot_and_return_image(fig)


    return image_bytes




# detect_grasp_part(grasp_part)
# detect_grasp_part('fork')                      # 11111



# plt.show()
# sys.exit()